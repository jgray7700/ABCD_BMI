library(glmnet)
library(mice)
library(caret)
library(moments)
library(psych)
library(dplyr)
##############

setwd("G:/")

#drop missing DV
myData <- myData[complete.cases(myData$bmiz95), ]

#drop unrealistic screentimes (>=18 hours per weekday or weekend 
myData[is.na(myData)] <- -999
myData <- myData[which(myData$screentime1_p_hours < 19),]
myData = myData[,!grepl("screentime1_p_hours",names(myData))]
myData <- myData[which(myData$screentime2_p_hours < 19),]
myData = myData[,!grepl("screentime2_p_hours",names(myData))]

#exclude participants who picked "don't know" for cash choice task
myData <- myData[which(myData$cash_choice_task < 3),]
myData[myData==-999] <- NA
sum(is.na(myData$Screentime))
sum(is.na(myData$cash_choice_task))

##Remove all participants that are missing, did not know, or refused to answer race
myData <- myData[which(myData$Missing < 1),]
myData = myData[,!grepl("Missing",names(myData))]

#exclude participant(s) with missing data for sex
myData <- myData[complete.cases(myData$Sex),]

#Exclude rows(subjects) for missing categorical variables
myData <- myData[complete.cases(myData$demo_prnt_prtnr_v2), ]
myData <- myData[complete.cases(myData$demo_ethn_v2), ]
myData <- myData[complete.cases(myData$demo_prnt_marital_v2), ]

##Descriptive stats of missing values
NAcol <- which(colSums(is.na(myData)) > 0)
sort(colSums(sapply(myData[NAcol], is.na)), decreasing = TRUE)
cat('There are', length(NAcol), 'columns with missing values')

####PERCENT MISSING APPROACH ######
#Calculate percent missing rows
percentmiss = function(x) { sum(is.na(x)) / length(x) * 100 }
missingsubjects = apply(myData,1,percentmiss)
summary(missingsubjects)
table(missingsubjects)

#Calculate percent missing columns
apply(myData,2,percentmiss)

#Exclude rows (subjects) missing > 5%
validpeople = myData[ missingsubjects < 5.01, ]
excludedpeople = myData[ missingsubjects > 5, ]

#Calculate new percent missing columns
missingcolumns = apply(validpeople,2,percentmiss)
summary(missingcolumns)
table(missingcolumns)
missingcolumns

#first exclude predictors you don't want influencing imputation
init = mice(validpeople, maxit=0)
meth = init$method
predM = init$predictorMatrix
predM[, c("bmiz95")]=0

validpeople$cash_choice_task=as.factor(validpeople$cash_choice_task)
meth[c("cash_choice_task")]="polyreg" # rest of variables will still be imputed with predictive mean matching (pmm) by default

#impute
set.seed(103)
imp.myData <- mice(validpeople, method=meth, predictorMatrix=predM, mech = "MNAR") #only use MNAR if think values are not missing at random
imp.myData$meth
densityplot(imp.myData, ~pea_wiscv_tss) ## This plot compares the density of observed data with the ones of imputed data. We expect them
# to be similar (though not identical) under MAR assumption. LOOK OUT FOR IMPOSSIBLE VALUES (e.g., negative BMI)
densityplot(imp.myData, ~demo_comb_income_v2) 
imp.myData <- complete(imp.myData)

myData <- imp.myData

#change cash_choice_task to integer to it can be include in further computatinons
myData$cash_choice_task=as.integer(myData$cash_choice_task)
sapply(myData, class)

#Normalizing data
skewness(myData)
#kurtosis = moment which means to define excess (to equal SPSS) you need to - 3
kurtosis(myData)-3

skew(myData$Screentime) 
skew(myData$cbcl_scr_syn_anxdep_t)
skew(myData$cbcl_scr_syn_withdep_t)
skew(myData$cbcl_scr_syn_social_t)
skew(myData$cbcl_scr_syn_thought_t)
skew(myData$cbcl_scr_syn_attention_t)
skew(myData$cbcl_scr_syn_rulebreak_t)
skew(myData$cbcl_scr_syn_aggressive_t)

qqnorm(myData$Screentime);qqline(myData$Screentime,col=2)

myData$Screentime <- myData$Screentime+1
myData$Screentime <- sqrt(myData$Screentime) #square root improves
myData$cbcl_scr_syn_anxdep_t <- log(myData$cbcl_scr_syn_anxdep_t)
myData$cbcl_scr_syn_withdep_t <- log(myData$cbcl_scr_syn_withdep_t)
myData$cbcl_scr_syn_social_t <- log(myData$cbcl_scr_syn_social_t)
myData$cbcl_scr_syn_thought_t <- log(myData$cbcl_scr_syn_thought_t)
myData$cbcl_scr_syn_attention_t <- log(myData$cbcl_scr_syn_attention_t)
myData$cbcl_scr_syn_rulebreak_t <- log(myData$cbcl_scr_syn_rulebreak_t)
myData$cbcl_scr_syn_aggressive_t <- log(myData$cbcl_scr_syn_aggressive_t)

skewness(myData)
kurtosis(myData)-3

#Adjust binary variables to 0 and 1; Note: Hispanic ethnicity (demo_ethn_v2) and White race (White_Binary) coefficients were *-1 in the results to facilitate interpretation.
myData$demo_prnt_prtnr_v2 <- myData$demo_prnt_prtnr_v2-1
myData$demo_prnt_marital_v2<- myData$demo_prnt_marital_v2-1
myData$demo_ethn_v2 <- myData$demo_ethn_v2-1
myData$cash_choice_task <- myData$cash_choice_task-1

###Standardize non-binary values prior to running regressions
myData[c(-1,-3,-22,-33,-34,-37,-38,-39,-40,-43)] <- scale(myData[c(-1,-3,-22,-33,-34,-37,-38,-39,-40,-43)]) #standardize all except for column 1 and 2 (outcome and sex)

#Save dataset for AfAmer analyses
myDataAfAmer <- myData
myData$Black_Binary <- NULL

n_occur <- data.frame(table(myData$rel_family_id))
sibFamId <- n_occur[n_occur$Freq > 1,]
head(sibFamId)

names(sibFamId)[1] <- "rel_family_id"
myDataNoSib <- myData[ ! myData$rel_family_id %in% sibFamId$rel_family_id, ]

#Data partition of participants with no siblings in the study
set.seed(155)
ind <- sample(2, nrow(myDataNoSib), replace = T, prob = c(.7,.3)) #70% training, 30% test data
train <- myDataNoSib[ind==1,]
test <- myDataNoSib[ind==2,]

#Data partition of family IDs for families w/ >1 children in the study
ind <- sample(2, nrow(sibFamId), replace = T, prob = c(.7,.3)) #70% training, 30% test data
trainFamId <- sibFamId[ind==1,]
testFamId <- sibFamId[ind==2,]

#Extract subjects with siblings and place whole family in either training or test set
myDataTrain <- myData[ myData$rel_family_id %in% trainFamId$rel_family_id, ]
myDataTest <- myData[ myData$rel_family_id %in% testFamId$rel_family_id, ]

train<-rbind(train, myDataTrain)
test<-rbind(test, myDataTest)

#drop relfamilyID
train$rel_family_id <- NULL
test$rel_family_id <- NULL
myData$rel_family_id <- NULL
myDataAfAmer$rel_family_id <- NULL
traincopy<-train
testcopy<-test

###Run models###
#Custom Control Parameters; number = number of folds, repeats = numer of complete sets of folds to compute
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5, verboseIter = T) #cross validation
# Elastic Net Regression
en <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=seq(0,1, length = 10), lambda = seq(.00001, 1, length = 5)), trControl = custom, standardize = T)
#Ridge Regression
ridge <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=0, lambda = seq(.00001, 1, length = 5)), trControl = custom)
#LASSO
lasso <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=1, lambda = seq(.00001, 1, length = 5)), trControl = custom)
#Linear Regression Model
lm <- train(bmiz95 ~ ., train, method = 'lm', trControl = custom) 

#Compare varImp
plot(varImp(en))
plot(varImp(ridge))
plot(varImp(lasso))
plot(varImp(lm))
#customizable shortened plot
plot(varImp(lasso), top = 26, lcolor = Orange)

#Merge and save coef
lmcoef <- coef(lm$finalModel, s = lm)
lmcoef <- as.matrix(lmcoef)
encoef <- coef(en$finalModel, s = en$bestTune$lambda)
encoef <- as.matrix(encoef)
ridgecoef <- coef(ridge$finalModel, s = ridge$bestTune$lambda)
ridgecoef <- as.matrix(ridgecoef)
lassocoef <- coef(lasso$finalModel, s = lasso$bestTune$lambda)
lassocoef <- as.matrix(lassocoef)
FullSampleCoef <- data.frame(lmcoef, encoef, ridgecoef, lassocoef)
names(FullSampleCoef) <- c("lmcoef", "encoef", "ridgecoef", "lassocoef")
write.table(FullSampleCoef, "FullSampleCoef.txt")

#Compare models
model_list <- list(LinearModel = lm, Ridge = ridge, Lasso = lasso, ElasticNet = en)
res <- resamples(model_list)
summary(res)
capture.output(summary(res), file = "FullSampleTrainModels.txt")

#Make predictions on the test data
testmodel <- model.matrix(bmiz95 ~ ., test)
enpredictions <- en %>% predict(testmodel)
ridgepredictions <- ridge %>% predict(testmodel)
lassopredictions <- lasso %>% predict(testmodel)
lmpredictions <- lm %>% predict(testmodel)

#Model performance metrics
entest <- data.frame(RMSE = RMSE(enpredictions, test$bmiz95),
                     Rsquare = R2(enpredictions, test$bmiz95))
ridgetest <- data.frame(RMSE = RMSE(ridgepredictions, test$bmiz95),
                        Rsquare = R2(ridgepredictions, test$bmiz95))
lassotest <- data.frame(RMSE = RMSE(lassopredictions, test$bmiz95),
                        Rsquare = R2(lassopredictions, test$bmiz95))
lmtest <- data.frame(RMSE = RMSE(lmpredictions, test$bmiz95),
                     Rsquare = R2(lmpredictions, test$bmiz95))
FullSampleTestModels <- data.frame(lmtest, entest, ridgetest, lassotest)
names(FullSampleTestModels) <- c("lmRMSE", "lmR2", "enRMSE", "enR2","ridgeRMSE", "ridgeR2", "lassoRMSE", "lassoR2")
write.table(FullSampleTestModels, "FullSampleTestModels.txt")

###SEPARATE MODELS BY SEX, RACE AND ETHNICITY
###################
#only include girls
train <- traincopy
train <- train %>% filter(Sex == 1)
train = train[,!grepl("Sex",names(train))]
test <- testcopy
test <- test %>% filter(Sex == 1)
test = test[,!grepl("Sex",names(test))]
###Run models###
#Custom Control Parameters; number = number of folds, repeats = numer of complete sets of folds to compute
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5, verboseIter = T) #cross validation
# Elastic Net Regression
en <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=seq(0,1, length = 10), lambda = seq(.00001, 1, length = 5)), trControl = custom, standardize = T)
#Ridge Regression
ridge <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=0, lambda = seq(.00001, 1, length = 5)), trControl = custom)
#LASSO
lasso <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=1, lambda = seq(.00001, 1, length = 5)), trControl = custom)
#Linear Regression Model
lm <- train(bmiz95 ~ ., train, method = 'lm', trControl = custom) 

#Compare varImp
plot(varImp(en))
plot(varImp(ridge))
plot(varImp(lasso))
plot(varImp(lm))
#customizable shortened plot
plot(varImp(lasso), top = 26, lcolor = Orange)

#Merge and save coef
lmcoef <- coef(lm$finalModel, s = lm)
lmcoef <- as.matrix(lmcoef)
encoef <- coef(en$finalModel, s = en$bestTune$lambda)
encoef <- as.matrix(encoef)
ridgecoef <- coef(ridge$finalModel, s = ridge$bestTune$lambda)
ridgecoef <- as.matrix(ridgecoef)
lassocoef <- coef(lasso$finalModel, s = lasso$bestTune$lambda)
lassocoef <- as.matrix(lassocoef)
GirlsSampleCoef <- data.frame(lmcoef, encoef, ridgecoef, lassocoef)
names(GirlsSampleCoef) <- c("lmcoef", "encoef", "ridgecoef", "lassocoef")
write.table(GirlsSampleCoef, "GirlsSampleCoef.txt")

#Compare models
model_list <- list(LinearModel = lm, Ridge = ridge, Lasso = lasso, ElasticNet = en)
res <- resamples(model_list)
summary(res)
capture.output(summary(res), file = "GirlsSampleTrainModels.txt")

#Make predictions on the test data
testmodel <- model.matrix(bmiz95 ~ ., test)
enpredictions <- en %>% predict(testmodel)
ridgepredictions <- ridge %>% predict(testmodel)
lassopredictions <- lasso %>% predict(testmodel)
lmpredictions <- lm %>% predict(testmodel)

#Model performance metrics
entest <- data.frame(RMSE = RMSE(enpredictions, test$bmiz95),
                     Rsquare = R2(enpredictions, test$bmiz95))
ridgetest <- data.frame(RMSE = RMSE(ridgepredictions, test$bmiz95),
                        Rsquare = R2(ridgepredictions, test$bmiz95))
lassotest <- data.frame(RMSE = RMSE(lassopredictions, test$bmiz95),
                        Rsquare = R2(lassopredictions, test$bmiz95))
lmtest <- data.frame(RMSE = RMSE(lmpredictions, test$bmiz95),
                     Rsquare = R2(lmpredictions, test$bmiz95))
GirlsSampleTestModels <- data.frame(lmtest, entest, ridgetest, lassotest)
names(GirlsSampleTestModels) <- c("lmRMSE", "lmR2", "enRMSE", "enR2","ridgeRMSE", "ridgeR2", "lassoRMSE", "lassoR2")
write.table(GirlsSampleTestModels, "GirlsSampleTestModels.txt")

###################
#only include boys
train = traincopy
train <- train %>% filter(Sex == 0)
train = train[,!grepl("Sex",names(train))]
test = testcopy
test <- test %>% filter(Sex == 0)
test = test[,!grepl("Sex",names(test))]
###Run models###
#Custom Control Parameters; number = number of folds, repeats = numer of complete sets of folds to compute
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5, verboseIter = T) #cross validation
# Elastic Net Regression
en <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=seq(0,1, length = 10), lambda = seq(.00001, 1, length = 5)), trControl = custom, standardize = T)
#Ridge Regression
ridge <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=0, lambda = seq(.00001, 1, length = 5)), trControl = custom)
#LASSO
lasso <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=1, lambda = seq(.00001, 1, length = 5)), trControl = custom)
#Linear Regression Model
lm <- train(bmiz95 ~ ., train, method = 'lm', trControl = custom) 

#Compare varImp
plot(varImp(en))
plot(varImp(ridge))
plot(varImp(lasso))
plot(varImp(lm))
#customizable shortened plot
plot(varImp(lasso), top = 26, lcolor = Orange)

#Merge and save coef
lmcoef <- coef(lm$finalModel, s = lm)
lmcoef <- as.matrix(lmcoef)
encoef <- coef(en$finalModel, s = en$bestTune$lambda)
encoef <- as.matrix(encoef)
ridgecoef <- coef(ridge$finalModel, s = ridge$bestTune$lambda)
ridgecoef <- as.matrix(ridgecoef)
lassocoef <- coef(lasso$finalModel, s = lasso$bestTune$lambda)
lassocoef <- as.matrix(lassocoef)
BoysSampleCoef <- data.frame(lmcoef, encoef, ridgecoef, lassocoef)
names(BoysSampleCoef) <- c("lmcoef", "encoef", "ridgecoef", "lassocoef")
write.table(BoysSampleCoef, "BoysSampleCoef.txt")

#Compare models
model_list <- list(LinearModel = lm, Ridge = ridge, Lasso = lasso, ElasticNet = en)
res <- resamples(model_list)
summary(res)
capture.output(summary(res), file = "BoysSampleTrainModels.txt")

#Make predictions on the test data
testmodel <- model.matrix(bmiz95 ~ ., test)
enpredictions <- en %>% predict(testmodel)
ridgepredictions <- ridge %>% predict(testmodel)
lassopredictions <- lasso %>% predict(testmodel)
lmpredictions <- lm %>% predict(testmodel)

#Model performance metrics
entest <- data.frame(RMSE = RMSE(enpredictions, test$bmiz95),
                     Rsquare = R2(enpredictions, test$bmiz95))
ridgetest <- data.frame(RMSE = RMSE(ridgepredictions, test$bmiz95),
                        Rsquare = R2(ridgepredictions, test$bmiz95))
lassotest <- data.frame(RMSE = RMSE(lassopredictions, test$bmiz95),
                        Rsquare = R2(lassopredictions, test$bmiz95))
lmtest <- data.frame(RMSE = RMSE(lmpredictions, test$bmiz95),
                     Rsquare = R2(lmpredictions, test$bmiz95))
BoysSampleTestModels <- data.frame(lmtest, entest, ridgetest, lassotest)
names(BoysSampleTestModels) <- c("lmRMSE", "lmR2", "enRMSE", "enR2","ridgeRMSE", "ridgeR2", "lassoRMSE", "lassoR2")
write.table(BoysSampleTestModels, "BoysSampleTestModels.txt")

###################
#only include white non-hispanic participants in train dataset equal to that of AfAmer sample
train = myData
train <- train %>% filter(White_Binary == 0)
train <- train %>% filter(demo_ethn_v2 == 1)
train = train[,!grepl("demo_ethn_v2",names(train))]
train = train[,!grepl("White_Binary",names(train))]
train <- sample_n(train, 366) #To match number of AfAmer Subj
###Run models###
#Custom Control Parameters; number = number of folds, repeats = numer of complete sets of folds to compute
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5, verboseIter = T) #cross validation
# Elastic Net Regression
en <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=seq(0,1, length = 10), lambda = seq(.00001, 1, length = 5)), trControl = custom, standardize = T)
#Ridge Regression
ridge <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=0, lambda = seq(.00001, 1, length = 5)), trControl = custom)
#LASSO
lasso <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=1, lambda = seq(.00001, 1, length = 5)), trControl = custom)
#Linear Regression Model
lm <- train(bmiz95 ~ ., train, method = 'lm', trControl = custom) 

#Compare varImp
plot(varImp(en))
plot(varImp(ridge))
plot(varImp(lasso))
plot(varImp(lm))
#customizable shortened plot
plot(varImp(lasso), top = 26, lcolor = Orange)

#Merge and save coef
lmcoef <- coef(lm$finalModel, s = lm)
lmcoef <- as.matrix(lmcoef)
encoef <- coef(en$finalModel, s = en$bestTune$lambda)
encoef <- as.matrix(encoef)
ridgecoef <- coef(ridge$finalModel, s = ridge$bestTune$lambda)
ridgecoef <- as.matrix(ridgecoef)
lassocoef <- coef(lasso$finalModel, s = lasso$bestTune$lambda)
lassocoef <- as.matrix(lassocoef)
EASampleCoefHisp <- data.frame(lmcoef, encoef, ridgecoef, lassocoef)
names(EASampleCoefHisp) <- c("lmcoef", "encoef", "ridgecoef", "lassocoef")
write.table(EASampleCoefHisp, "EASampleCoefAfAmer.txt")

#Compare models
model_list <- list(LinearModel = lm, Ridge = ridge, Lasso = lasso, ElasticNet = en)
res <- resamples(model_list)
summary(res)
capture.output(summary(res), file = "EASampleAfAmerTrainModels.txt")

###################
#only include white non-hispanic participants in train dataset equal to that of Hispanic sample
train = myData
train <- train %>% filter(White_Binary == 0)
train <- train %>% filter(demo_ethn_v2 == 1)
train = train[,!grepl("demo_ethn_v2",names(train))]
train = train[,!grepl("White_Binary",names(train))]
train <- sample_n(train, 727) #To match number of Hispanic sample
###Run models###
#Custom Control Parameters; number = number of folds, repeats = numer of complete sets of folds to compute
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5, verboseIter = T) #cross validation
# Elastic Net Regression
en <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=seq(0,1, length = 10), lambda = seq(.00001, 1, length = 5)), trControl = custom, standardize = T)
#Ridge Regression
ridge <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=0, lambda = seq(.00001, 1, length = 5)), trControl = custom)
#LASSO
lasso <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=1, lambda = seq(.00001, 1, length = 5)), trControl = custom)
#Linear Regression Model
lm <- train(bmiz95 ~ ., train, method = 'lm', trControl = custom) 

#Compare varImp
plot(varImp(en))
plot(varImp(ridge))
plot(varImp(lasso))
plot(varImp(lm))
#customizable shortened plot
plot(varImp(lasso), top = 26, lcolor = Orange)

#Merge and save coef
lmcoef <- coef(lm$finalModel, s = lm)
lmcoef <- as.matrix(lmcoef)
encoef <- coef(en$finalModel, s = en$bestTune$lambda)
encoef <- as.matrix(encoef)
ridgecoef <- coef(ridge$finalModel, s = ridge$bestTune$lambda)
ridgecoef <- as.matrix(ridgecoef)
lassocoef <- coef(lasso$finalModel, s = lasso$bestTune$lambda)
lassocoef <- as.matrix(lassocoef)
EASampleCoefAfAmer <- data.frame(lmcoef, encoef, ridgecoef, lassocoef)
names(EASampleCoefAfAmer) <- c("lmcoef", "encoef", "ridgecoef", "lassocoef")
write.table(EASampleCoefAfAmer, "EASampleCoefHisp.txt")

#Compare models
model_list <- list(LinearModel = lm, Ridge = ridge, Lasso = lasso, ElasticNet = en)
res <- resamples(model_list)
summary(res)
capture.output(summary(res), file = "EASampleHispTrainModels.txt")

###################
#only include black non-hispanic participants
myDataAfAmer <- myDataAfAmer %>% filter(Black_Binary == 0)
myDataAfAmer <- myDataAfAmer %>% filter(demo_ethn_v2 == 1)
myDataAfAmer = myDataAfAmer[,!grepl("demo_ethn_v2",names(myDataAfAmer))]
myDataAfAmer = myDataAfAmer[,!grepl("Black_Binary",names(myDataAfAmer))]
myDataAfAmer = myDataAfAmer[,!grepl("White_Binary",names(myDataAfAmer))]
train <- myDataAfAmer
###Run models###
#Custom Control Parameters; number = number of folds, repeats = numer of complete sets of folds to compute
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5, verboseIter = T) #cross validation
# Elastic Net Regression
en <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=seq(0,1, length = 10), lambda = seq(.00001, 1, length = 5)), trControl = custom, standardize = T)
#Ridge Regression
ridge <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=0, lambda = seq(.00001, 1, length = 5)), trControl = custom)
#LASSO
lasso <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=1, lambda = seq(.00001, 1, length = 5)), trControl = custom)
#Linear Regression Model
lm <- train(bmiz95 ~ ., train, method = 'lm', trControl = custom) 

#Compare varImp
plot(varImp(en))
plot(varImp(ridge))
plot(varImp(lasso))
plot(varImp(lm))
#customizable shortened plot
plot(varImp(lasso), top = 26, lcolor = Orange)

#Merge and save coef
lmcoef <- coef(lm$finalModel, s = lm)
lmcoef <- as.matrix(lmcoef)
encoef <- coef(en$finalModel, s = en$bestTune$lambda)
encoef <- as.matrix(encoef)
ridgecoef <- coef(ridge$finalModel, s = ridge$bestTune$lambda)
ridgecoef <- as.matrix(ridgecoef)
lassocoef <- coef(lasso$finalModel, s = lasso$bestTune$lambda)
lassocoef <- as.matrix(lassocoef)
AfAmerSampleCoef <- data.frame(lmcoef, encoef, ridgecoef, lassocoef)
names(AfAmerSampleCoef) <- c("lmcoef", "encoef", "ridgecoef", "lassocoef")
write.table(AfAmerSampleCoef, "AfAmerSampleCoef.txt")

#Compare models
model_list <- list(LinearModel = lm, Ridge = ridge, Lasso = lasso, ElasticNet = en)
res <- resamples(model_list)
summary(res)
capture.output(summary(res), file = "AfAmerSampleTrainModels.txt")

###################
#only include hispanic participants
myDataHisp <- myData %>% filter(demo_ethn_v2 == 0)
myDataHisp = myDataHisp[,!grepl("demo_ethn_v2",names(myDataHisp))]
train <- myDataHisp

###Run models###
#Custom Control Parameters; number = number of folds, repeats = numer of complete sets of folds to compute
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5, verboseIter = T) #cross validation
# Elastic Net Regression
en <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=seq(0,1, length = 10), lambda = seq(.00001, 1, length = 5)), trControl = custom, standardize = T)
#Ridge Regression
ridge <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=0, lambda = seq(.00001, 1, length = 5)), trControl = custom)
#LASSO
lasso <- train(bmiz95 ~ ., train, method = 'glmnet', tuneGrid = expand.grid(alpha=1, lambda = seq(.00001, 1, length = 5)), trControl = custom)
#Linear Regression Model
lm <- train(bmiz95 ~ ., train, method = 'lm', trControl = custom) 

#Compare varImp
plot(varImp(en))
plot(varImp(ridge))
plot(varImp(lasso))
plot(varImp(lm))
#customizable shortened plot
plot(varImp(lasso), top = 26, lcolor = Orange)

#Merge and save coef
lmcoef <- coef(lm$finalModel, s = lm)
lmcoef <- as.matrix(lmcoef)
encoef <- coef(en$finalModel, s = en$bestTune$lambda)
encoef <- as.matrix(encoef)
ridgecoef <- coef(ridge$finalModel, s = ridge$bestTune$lambda)
ridgecoef <- as.matrix(ridgecoef)
lassocoef <- coef(lasso$finalModel, s = lasso$bestTune$lambda)
lassocoef <- as.matrix(lassocoef)
HispSampleCoef <- data.frame(lmcoef, encoef, ridgecoef, lassocoef)
names(HispSampleCoef) <- c("lmcoef", "encoef", "ridgecoef", "lassocoef")
write.table(HispSampleCoef, "HispSampleCoef.txt")

#Compare models
model_list <- list(LinearModel = lm, Ridge = ridge, Lasso = lasso, ElasticNet = en)
res <- resamples(model_list)
summary(res)
capture.output(summary(res), file = "HispSampleTrainModels.txt")
